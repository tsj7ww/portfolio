{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid19 Data Preprocessing\n",
    "\n",
    "### ToDo\n",
    "1. Add population per 1k to datasets\n",
    "1. Get the data types correct\n",
    "1. Create cumulative & non-cumulative columns for time series cases & deaths\n",
    "\n",
    "### Overview\n",
    "...\n",
    "\n",
    "### Data Sources\n",
    "* **Covid19**\n",
    "    * [Johns Hopkins University - Covid19 Data](https://github.com/CSSEGISandData/COVID-19)\n",
    "* **Demographics**\n",
    "    * [US Census Bureau - 2019 Annual Social and Economic Supplements](https://www.census.gov/content/census/en/data/datasets/2019/demo/cps/cps-asec-2019.html)\n",
    "    * [US Census Bureau - 2019 Current Population Survey](https://www.census.gov/content/census/en/data/datasets/2019/demo/cps/cps-basic-2019.html)\n",
    "    * [US Census Bureau - International Demographic Overview](https://www.census.gov/data-tools/demo/idb/region.php?T=13&RT=0&A=both&Y=2020&C=&R=1)\n",
    "    * [The World Bank - World Development Indicators](https://datacatalog.worldbank.org/dataset/world-development-indicators)\n",
    "    * [The World Bank - Population Estimates and Projections](https://datacatalog.worldbank.org/dataset/population-estimates-and-projections)\n",
    "    * [IMF - World Economic Outlook](https://www.imf.org/external/pubs/ft/weo/2020/01/weodata/download.aspx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import subprocess\n",
    "import os\n",
    "from io import StringIO\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import requests as rq\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process:\n",
    "    def __init__(self):\n",
    "        self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CONFIG(env):\n",
    "    \"\"\"\"\"\"\n",
    "    pid = 'analysis-covid19'\n",
    "    now = datetime.datetime.now()\n",
    "    cwd = os.getcwd()\n",
    "    f_in = os.path.join(cwd,'in')\n",
    "    f_out = os.path.join(cwd,'out')\n",
    "    cov = os.path.join(f_in,'covid')\n",
    "    \n",
    "    cfg = {\n",
    "        'paths': {\n",
    "            'covid': cov\n",
    "            ,'f_in': f_in\n",
    "            ,'f_out': f_out\n",
    "            ,'eda': os.path.join(cwd,'eda')\n",
    "            ,'pq': os.path.join(f_out)\n",
    "        }\n",
    "        ,'jhu-refresh': os.path.join(cwd,'jhu-refresh.sh')\n",
    "        ,'jhu-dly': { # johns hopkins university - global daily\n",
    "            'path': os.path.join(cov,'jhu','csse-dly','{DATE}.csv')\n",
    "            ,'dates': {\n",
    "                'start': datetime.date(2020,4,12)\n",
    "                ,'end': datetime.date(2020,9,30)\n",
    "            }\n",
    "        }\n",
    "        ,'jhu-dly-us': { # johns hopkins university - us daily\n",
    "            'path': os.path.join(cov,'jhu','csse-dly-us','{DATE}.csv')\n",
    "            ,'dates': {\n",
    "                'start': datetime.date(2020,4,12)\n",
    "                ,'end': datetime.date(2020,9,30)\n",
    "            }\n",
    "        }\n",
    "        ,'jhu-ts': { # johns hopkins university - timeseries\n",
    "            'base': os.path.join(cov,'jhu','csse-ts','time_series_covid19_{STATUS}_{REGION}.csv')\n",
    "            ,'options': {\n",
    "                'status': ['confirmed','deaths','recovered']\n",
    "                ,'region': ['global','US']\n",
    "            }\n",
    "            ,'errata': os.path.join(cov,'jhu','csse-ts','Errata.csv')\n",
    "        }\n",
    "        ,'jhu-who-ts': { # johns hopkins university - WHO timeseries\n",
    "            'path': os.path.join(cov,'jhu','who-ts','who_covid_19_sit_rep_time_series.csv')\n",
    "        }\n",
    "        ,'jhu-fips': { # johns hopkins university - fips lkup table\n",
    "            'path': os.path.join(cov,'jhu','csse-fips-lkup.csv')\n",
    "        }\n",
    "        ,'cb-acs': { # census bureau - american community survey\n",
    "            'base': 'https://api.census.gov/data/{YEAR}/pep/charage?get={FIELDS}&for=state:{STATES}'\n",
    "            ,'query': {\n",
    "                'years': ['2019']\n",
    "                ,'fields': ','.join(['POP','NAME'])\n",
    "                ,'states': ','.join(['*'])\n",
    "            }\n",
    "        }\n",
    "        ,'cb-idb': { # census bureau - international database\n",
    "            'base': 'https://api.census.gov/data/timeseries/idb/1year?time={YEAR}&get={FIELDS}'\n",
    "            ,'query': {\n",
    "                'years': ['2019']\n",
    "                ,'fields': ['AREA_KM2','NAME','AGE','POP','FIPS','SEX']\n",
    "            }\n",
    "        }\n",
    "        ,'cb-geo': { # census bureau - county pop density data\n",
    "            'base': 'https://opendata.arcgis.com/datasets' # /\n",
    "            ,'fname': '21843f238cbb46b08615fc53e19e0daf_1.geojson'\n",
    "        }\n",
    "        ,'cb-pov': { # census bureau - county poverty data\n",
    "            'url': 'https://www.census.gov/cgi-bin/nbroker?_service=sas_serv1&_debug=0&_program=cedr.sasapp_main.sas&s_appName=saipe&map_geoSelector=aa_c&s_year=2019&s_measures=aa_snc&s_state=&s_county=&s_district=&menu=grid&s_output=csv&s_orderBy=id%20asc,year%20desc'\n",
    "              # 2020-03-01 unable to feed params into get request without error\n",
    "#             'base': 'https://www.census.gov/cgi-bin/nbroker' # ?\n",
    "#             ,'params': {\n",
    "#                 '_service': 'sas_serv1',\n",
    "#                 '_debug': '0',\n",
    "#                 '_program': 'cedr.sasapp_main.sas',\n",
    "#                 's_appName': 'saipe',\n",
    "#                 's_measures': 'aa_snc',\n",
    "#                 's_state': '',\n",
    "#                 's_county': '',\n",
    "#                 's_district': '',\n",
    "#                 'map_yearSelector': '2019',\n",
    "#                 'map_geoSelector': 'aa_c',\n",
    "#                 's_year': '2019',\n",
    "#                 'menu': 'grid',\n",
    "#                 's_output': 'csv',\n",
    "#                 's_orderBy': 'id%20asc,year%20desc',\n",
    "#             } # join with &\n",
    "        }\n",
    "#         ,'wb-dev': {\n",
    "#             'base'\n",
    "#         }\n",
    "#         ,'wb-pop': {\n",
    "            \n",
    "#         }\n",
    "#         ,'imf-econ': {\n",
    "            \n",
    "#         }\n",
    "    }\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JHU_DLY(base,start,end):\n",
    "    \"\"\"\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    file_dt = '%m-%d-%Y'\n",
    "    data_dt = '%Y-%m-%d'\n",
    "    \n",
    "    df = pd.DataFrame(None)\n",
    "    errs = []\n",
    "    \n",
    "    for i in range((end-start).days+1):\n",
    "        dt = (start+datetime.timedelta(days=i))\n",
    "        try:\n",
    "            if df.empty:\n",
    "                df = pd.read_csv(base.format(DATE=dt.strftime(file_dt)),header=0,dtype=object).assign(DATA_DT=dt.strftime(data_dt))\n",
    "            else:\n",
    "                df = df.append(pd.read_csv(base.format(DATE=dt.strftime(file_dt)),header=0,dtype=object).assign(DATA_DT=dt.strftime(data_dt)))\n",
    "        except Exception as e:\n",
    "            errs.append((dt,str(e)))\n",
    "                               \n",
    "    return df,errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CB_ACS(base,years,fields,states):\n",
    "    \"\"\"\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    df = pd.DataFrame(None)\n",
    "    errs = []\n",
    "    \n",
    "    for yr in years:\n",
    "        url = base.format(YEAR=yr,FIELDS=fields,STATES=states)\n",
    "        response = rq.get(url)\n",
    "        if response.status_code == rq.codes.ok:\n",
    "            data = response.json()\n",
    "            if df.empty:\n",
    "                df = pd.DataFrame(data[1:],columns=data[0])\n",
    "            else:\n",
    "                df.append(pd.DataFrame(data[1:],columns=data[0]))\n",
    "        else:\n",
    "            errs.append(url)\n",
    "    \n",
    "    return df,errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CB_IDB(base,years,fields):\n",
    "    \"\"\"\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    df = pd.DataFrame(None)\n",
    "    errs = []\n",
    "    \n",
    "    for yr in years:\n",
    "        url = base.format(YEAR=yr,FIELDS=','.join(fields))\n",
    "        response = rq.get(url)\n",
    "        if response.status_code == rq.codes.ok:\n",
    "            data = response.json()\n",
    "            if df.empty:\n",
    "                df = pd.DataFrame(data[1:],columns=data[0])\n",
    "            else:\n",
    "                df.append(pd.DataFrame(data[1:],columns=data[0]))\n",
    "        else:\n",
    "            errs.append(url)\n",
    "    \n",
    "    return df,errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CB_GEO(base,fname):\n",
    "    \"\"\"\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    errs = []\n",
    "    \n",
    "    url = os.path.join(base,fname)\n",
    "    response = rq.get(url)\n",
    "    if response.status_code == rq.codes.ok:\n",
    "        content = response.json()\n",
    "        data = [feature['properties'] for feature in content['features']]\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        errs.append(url)\n",
    "    \n",
    "    return df,errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CB_POV(base,params):\n",
    "def CB_POV(url):\n",
    "    \"\"\"\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    errs = []\n",
    "    \n",
    "#     url = '{}?{}'.format(base,'&'.join(params))\n",
    "    response = rq.get(url)\n",
    "    if response.status_code == rq.codes.ok:\n",
    "        decode = response.content.decode('utf-8')\n",
    "        data = list(csv.reader(decode.splitlines(), delimiter=','))\n",
    "        df = pd.DataFrame(data[1:],columns=data[0])\n",
    "    else:\n",
    "        errs.append(url)\n",
    "    \n",
    "    return df,errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLEANER(df):\n",
    "    \"\"\"\"\"\"\n",
    "    def COLUMN(x):\n",
    "        if not x[0].isalnum():\n",
    "            x=x[1:]\n",
    "        if not x[-1].isalnum():\n",
    "            x=x[:-1]\n",
    "        return x.upper()\n",
    "\n",
    "    df.columns = map(lambda x: COLUMN(x), df.columns)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    fips = ['GEOID','FIPS','COUNTY_ID']\n",
    "    state = ['PROVINCE_STATE']\n",
    "    for i in df.columns:\n",
    "        if i in fips:\n",
    "            try:\n",
    "                df.loc[:,i] = df.loc[:,i].astype(int).astype(str)\n",
    "            except:\n",
    "                pass\n",
    "        if i in state:\n",
    "            df.loc[:,i] = df.loc[:,i].str.upper()    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNPIVOT(df, segment):\n",
    "    \"\"\"\n",
    "    Unpivot JHU Covid19 timeseries data\n",
    "    \"\"\"\n",
    "    \n",
    "    unpivot = [i for i in df.columns if re.match('\\d{1,2}\\/\\d{1,2}\\/\\d{2}',i)]\n",
    "    static = [i for i in df.columns if i not in unpivot]\n",
    "    \n",
    "    df = pd.melt(df, id_vars=static, value_vars=unpivot, var_name='RECORD_DT', value_name=segment)\n",
    "    df.RECORD_DT = pd.to_datetime(df.RECORD_DT)\n",
    "    \n",
    "#     if segment == 'CASES':\n",
    "#         df = df.sort_values(by=['RECORD_DT']).reset_index(drop=True)\n",
    "#         df['CASES_CUM'] = df.groupby(['FIPS'])['CASES'].cumsum(axis=0)\n",
    "    \n",
    "    return df.sort_values(by=['FIPS','RECORD_DT']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA(df,f_out,n=100):\n",
    "    \"\"\"\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    name = os.path.basename(f_out).split('.')[0]\n",
    "    lb = '\\n'\n",
    "    lblb = '\\n\\n'\n",
    "    \n",
    "    # columns & types\n",
    "    content = '# EDA - {} Files {}'.format(os.path.basename(name).upper(),lblb)\n",
    "    content+='#### Column Name [IDX] -  Dtype (Head / Tail) \\n'\n",
    "    dtypes = df.dtypes.to_dict()\n",
    "    head = df.head(1).T.iloc[:,0].to_list() # to_dict() - head.get(j)\n",
    "    tail = df.tail(1).T.iloc[:,0].to_list() # to_dict() - tail.get(j)\n",
    "    for i,j in enumerate(df.columns):\n",
    "        content+='- **{}** [{}] - {} ({} / {}) {}'.format(j, i, dtypes.get(j), head[i], tail[i], lb)\n",
    "    \n",
    "    # html\n",
    "    content+='{}#### Head / Tail [n={}] Sample {}'.format(lb+lblb,n,lblb)\n",
    "    content+=(df.head(n).append(df.tail(n)).to_html(None,index=True,header=True))\n",
    "    \n",
    "    with open(f_out,'w') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PARQUET(df,f_out):\n",
    "    \"\"\"\"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    df.to_parquet(f_out,engine='pyarrow',index=False,compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "# def MAIN():\n",
    "#     \"\"\"\"\"\"\n",
    "    env = 'dev'\n",
    "    LOG,FEED = LOGGER(env)\n",
    "    LOG.info('Setting config for env {}'.format(env))\n",
    "    CFG = CONFIG(env)\n",
    "    LOG.info('Config set.')\n",
    "    sample_size = 20\n",
    "    refresh = False\n",
    "    \n",
    "    ########################\n",
    "    ### JHU DATA REFRESH ###\n",
    "    ########################\n",
    "    LOG.info('Refresh value: {}.'.format(refresh))\n",
    "    if refresh:\n",
    "        LOG.info('Refreshing JHU GitHub data.')\n",
    "        out = subprocess.run(['sh', CFG['jhu-refresh']]).returncode\n",
    "        if out == 0:\n",
    "            pass\n",
    "        elif out == 1:\n",
    "            raise Exception('Error while refreshing covid19 data.')\n",
    "        elif out == 127:\n",
    "            raise Exception('File not found.')\n",
    "        else:\n",
    "            raise Exception('Unknown shell out code: {}'.format(out))\n",
    "        \n",
    "    ########################\n",
    "    ### COVID19 JHU DATA ###\n",
    "    ########################\n",
    "    LOG.info('Starting Covid data.')\n",
    "    try:\n",
    "        segment = 'covid'\n",
    "        src = 'jhu'\n",
    "        LOG.info('Creating Covid dataframes.')\n",
    "        covid = {\n",
    "            'jhu-dly': JHU_DLY(CFG['jhu-dly']['path'],**CFG['jhu-dly']['dates'])[0],\n",
    "            'jhu-dly-us': JHU_DLY(CFG['jhu-dly-us']['path'],**CFG['jhu-dly-us']['dates'])[0],\n",
    "            'jhu-ts-deaths': pd.read_csv(CFG['jhu-ts']['base'].format(STATUS='deaths',REGION='US'),header=0,dtype=object),\n",
    "            'jhu-ts-cases': pd.read_csv(CFG['jhu-ts']['base'].format(STATUS='confirmed',REGION='US'),header=0,dtype=object),\n",
    "            #'jhu-who-ts': pd.read_csv(CFG['jhu-who-ts']['path'],header=0,dtype=object), # keep getting indexing error in EDA function\n",
    "            'jhu-ts-err': pd.read_csv(CFG['jhu-ts']['errata'],header=0,dtype=object),\n",
    "            'jhu-fips': pd.read_csv(CFG['jhu-fips']['path'],header=0,dtype=object),\n",
    "        }\n",
    "        for fname,df in covid.items():\n",
    "            LOG.info('Processing {}.'.format(fname))\n",
    "            # clean df\n",
    "            df = CLEANER(df)\n",
    "            # unpivot timeseries data\n",
    "            if fname in ['jhu-ts-cases','jhu-ts-deaths']:\n",
    "                df = UNPIVOT(df, fname.split('-')[-1].upper())\n",
    "            # md eda file\n",
    "            fpath = os.path.join(CFG['paths']['eda'],'{}-{}.md'.format(segment,fname))\n",
    "            EDA(df,fpath,n=sample_size)\n",
    "            # parquet clean file\n",
    "            fpath = os.path.join(CFG['paths']['pq'],segment,'{}-{}.parquet.gzip'.format(segment,fname))\n",
    "            PARQUET(df,fpath)\n",
    "    except Exception as e:\n",
    "        LOG.critical(str(e))\n",
    "        raise\n",
    "    LOG.info('Covid data processed.')\n",
    "    \n",
    "    ########################\n",
    "    ### DEMOGRAPHIC DATA ###\n",
    "    ########################\n",
    "    LOG.info('Starting demographic data.')\n",
    "    try:\n",
    "        segment = 'demo'\n",
    "        for fname in ['cb-acs','cb-idb','cb-geo','cb-pov']:\n",
    "            LOG.info('Processing {}.'.format(fname))\n",
    "            if fname == 'cb-acs':\n",
    "                df,errs = CB_ACS(CFG[fname]['base'],**CFG[fname]['query'])\n",
    "            elif fname == 'cb-idb':\n",
    "                df,errs = CB_IDB(CFG[fname]['base'],**CFG[fname]['query'])\n",
    "            elif fname == 'cb-geo':\n",
    "                df,errs = CB_GEO(**CFG[fname])\n",
    "            elif fname == 'cb-pov':\n",
    "                df,errs = CB_POV(**CFG[fname])\n",
    "            else:\n",
    "                raise Exception('Unknown file name.')\n",
    "            # md eda file\n",
    "            fpath = os.path.join(CFG['paths']['eda'],'{}-{}.md'.format(segment,fname))\n",
    "            EDA(df,fpath,n=sample_size)\n",
    "            # parquet clean file\n",
    "            fpath = os.path.join(CFG['paths']['pq'],segment,'{}-{}.parquet.gzip'.format(segment,fname))\n",
    "            PARQUET(df,fpath)\n",
    "        \n",
    "    LOG.info('Processing complete.')\n",
    "    \n",
    "    except Exception as e:\n",
    "        LOG.critical(str(e))\n",
    "        print(LOG.getvalue())\n",
    "        del LOG\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
